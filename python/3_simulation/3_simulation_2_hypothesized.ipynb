{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"../../\"\n",
    "plts_folder = root_folder + \"Plots/py/3_HypothesizedParticipantsPlots/\"\n",
    "vars_folder = root_folder + \"vars/py/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 200 hypothesized participants\n",
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "data = load_data_from_pickle(\"../../Data/res_py/Data_s2.pkl\")\n",
    "data = data[data['participant'] == 1]\n",
    "\n",
    "data = data.sort_values(by=['participant', 'trials'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda value\n",
    "Here we show how we decide the boundary of 位 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_d = data['d_phy_p'].max()\n",
    "min_d = sorted(data['d_phy_p'].unique())[1]\n",
    "\n",
    "# Create decay values table for different lambda values\n",
    "lambda_values = np.arange(0, 0.501, 0.001)\n",
    "sim_decay = pd.DataFrame({\n",
    "    'lambda': lambda_values,\n",
    "    'sim_max': np.exp(-lambda_values * max_d),\n",
    "    'sim_min': np.exp(-lambda_values * min_d)\n",
    "})\n",
    "\n",
    "# Define similarity thresholds\n",
    "min_perc = 0.7\n",
    "max_perc = 0.95\n",
    "\n",
    "# Calculate the minimum and maximum lambda values\n",
    "min_lam = round(-(np.log(min_perc) / max_d), 4)\n",
    "max_lam = round(-(np.log(max_perc) / min_d), 4)\n",
    "\n",
    "# Create decay table for different distances with min and max lambda values\n",
    "d_values = np.arange(0, 101, 1)\n",
    "sim_decay_2 = pd.DataFrame({\n",
    "    'd': d_values,\n",
    "    'sim_max': np.exp(-min_lam * d_values),\n",
    "    'sim_min': np.exp(-max_lam * d_values),\n",
    "    'sim_min_og': np.exp(0 * d_values)\n",
    "})\n",
    "\n",
    "# Plot decay of similarity for different lambda values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for sim_decay lambda values\n",
    "sns.lineplot(data=sim_decay, x='lambda', y='sim_max', ax=axes[0], linewidth=2)\n",
    "axes[0].axvline(min_lam, linestyle='--', color='grey')\n",
    "axes[0].axhline(min_perc, linestyle='--', color='grey')\n",
    "axes[0].plot(min_lam, min_perc, 'ro', markersize=8)\n",
    "axes[0].set_xlabel(\"位\")\n",
    "axes[0].set_ylabel(f\"Similarity given distance = {max_d}\")\n",
    "axes[0].text(min_lam + 0.1, min_perc + 0.05, f\"位 = {min_lam}\", color='red')\n",
    "\n",
    "# Plot for sim_decay_2 values by distance\n",
    "sns.lineplot(data=sim_decay_2, x='d', y='sim_max', ax=axes[1], color='black', linewidth=2)\n",
    "axes[1].text(40, 0.95, f\"位 = {min_lam}\", color='black')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_xticks(np.arange(0, 101, 25))\n",
    "axes[1].set_xlabel(\"Distance\")\n",
    "axes[1].set_ylabel(\"Similarity\")\n",
    "\n",
    "# Combine and save plots\n",
    "plt.suptitle(\"Decay of Similarity with Different Lambda Values\")\n",
    "plt.savefig(f\"{plts_folder}/Sim_lambda.pdf\", dpi=300, format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter settings\n",
    "We will simulate hypothesised participants in 4 latent groups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of participants in each group\n",
    "Ngroup1 = 50\n",
    "Ngroup2 = 50\n",
    "Ngroup3 = 50\n",
    "Ngroup4 = 50\n",
    "\n",
    "# Calculate the total number of participants\n",
    "Nparticipants = Ngroup1 + Ngroup2 + Ngroup3 + Ngroup4\n",
    "\n",
    "# Get the number of trials from the data\n",
    "Ntrials = int(data['trials'].max())\n",
    "\n",
    "# Set the number of actual trials (excluding warm-up and cool-down trials)\n",
    "Nactrials = 24\n",
    "\n",
    "# Define the group names and colors for plots\n",
    "group_nm = [\"Non-Learners\", \"Overgeneralizers\", \"Physical Generalizers\", \"Perceptual Generalizers\", \"Unknown\"]\n",
    "color_gp = [\"#CC79A7\", \"#F0E442\", \"#56B4E9\", \"#D55E00\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fun(var_key, var_val):\n",
    "    _x = {}\n",
    "    for x in range(Ntrials):\n",
    "        col_name = f\"{var_key}.{x + 1}\"\n",
    "        _col_data = []\n",
    "        for nc in range(Nparticipants):\n",
    "            _col_data.append(data[var_val].iloc[x])\n",
    "        # print(_col_data)\n",
    "        _x[col_name]= _col_data\n",
    "    return pd.DataFrame(_x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a new list 'variable_name' with named elements that correspond to the names of variables in the 'data' list\n",
    "variable_name = {\n",
    "  \"d_phy_p\" : \"d_phy_p\", \n",
    "  \"d_phy_m\" : \"d_phy_m\", \n",
    "  \"kplus\" :\"CSptrials\", \n",
    "  \"kminus\" : \"CSmtrials\", \n",
    "  \"rplus\" :\"USp\", \n",
    "  \"rminus\" :\"USm\"\n",
    "}\n",
    "# Apply 'extract_fun' to each element in 'variable_name' to create the 'variables' dictionary\n",
    "variables = pd.concat([extract_fun(var, variable_name[var]) for var in variable_name], axis=1)\n",
    "\n",
    "variables.to_csv(f\"{vars_folder}variables_py.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function to get the values of columns and make them in one column dataframe that I can use after, the count of each one should be 36,000 (180 * 200):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValuesOfCols(col_start):\n",
    "    _cols = [col for col in variables.columns if col.startswith(col_start)]\n",
    "    vals = pd.concat([variables[col] for col in _cols], ignore_index=True).to_frame(name=col_start)\n",
    "    return vals\n",
    "\n",
    "\n",
    "d_phy_p_vals = getValuesOfCols(\"d_phy_p\")\n",
    "d_phy_m_vals = getValuesOfCols(\"d_phy_m\")\n",
    "rplus_vals = getValuesOfCols(\"rplus\")\n",
    "\n",
    "vars = {\n",
    "    \"rplus\": np.empty((Nparticipants, Ntrials)),\n",
    "    \"kplus\": np.empty((Nparticipants, Ntrials)),\n",
    "    \"kminus\": np.empty((Nparticipants, Ntrials)),\n",
    "    \"rminus\": np.empty((Nparticipants, Ntrials)),\n",
    "    \"d_phy_p\": np.empty((Nparticipants, Ntrials)),\n",
    "    \"d_phy_m\": np.empty((Nparticipants, Ntrials)),\n",
    "    \n",
    "}\n",
    "\n",
    "for i in range(Nparticipants):\n",
    "    for j in range(Ntrials):\n",
    "        vars[\"rplus\"][i,j] = variables[f'rplus.{j + 1}'][i]\n",
    "        vars[\"kplus\"][i,j] = variables[f'kplus.{j + 1}'][i]\n",
    "        vars[\"kminus\"][i,j] = variables[f'kminus.{j + 1}'][i]\n",
    "        vars[\"rminus\"][i,j] = variables[f'rminus.{j + 1}'][i]\n",
    "        vars[\"d_phy_p\"][i,j] = variables[f'd_phy_p.{j + 1}'][i]\n",
    "        vars[\"d_phy_m\"][i,j] = variables[f'd_phy_m.{j + 1}'][i]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code define a python function called lg_fun() that appears to simulate a learning process. \n",
    "The function has three parameters: persd, A, and K. \n",
    "The persd parameter represents the perceived distance, and the A and K parameters appear to be constants.\n",
    "\n",
    "Inside the function, several matrices and vectors are initialized. \n",
    "The y matrix will hold the output of the learning process. \n",
    "The theta matrix will hold some intermediate values used in the learning process. \n",
    "The s_plus and s_minus matrices will hold similarity values. \n",
    "The v_plus and v_minus matrices will hold some other intermediate values used in the learning process. \n",
    "The g matrix will hold generalization values. \n",
    "The d_per_p and d_per_m matrices will hold perceived distances. \n",
    "The group vector will hold group membership information for each participant. \n",
    "The alpha, lambda, w0, w1, and sigma vectors will hold parameters that vary across participants.\n",
    "\n",
    "The function then loops over all participants and trials, and performs the following actions:\n",
    "1.Updates the v_plus and v_minus matrices based on learning. \n",
    "2. Calculates the s_plus and s_minus matrices based on similarity. \n",
    "3. Calculates the g matrix based on generalization. \n",
    "4. Calculates the theta matrix based on the g matrix and the A and K parameters. \n",
    "5. Calculates the y matrix based on the theta matrix and some other intermediate values. \n",
    "6. Finally, the function returns the y matrix as its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_plus = np.zeros((Nparticipants, Ntrials + 1))\n",
    "alpha = pd.read_csv('../../vars/randoms/alpha.csv').values.tolist()\n",
    "alpha = [it[0] for it in  alpha]\n",
    "group = np.repeat([1, 2, 3, 4], Nparticipants // 4)\n",
    "v_minus = np.zeros((Nparticipants, Ntrials + 1))\n",
    "for i in range(Nparticipants):\n",
    "    for j in range(Ntrials):\n",
    "        # Learning\n",
    "        v_plus[i, j + 1] = (\n",
    "            v_plus[i, j] + alpha[i] * ( vars['rplus'][i, j] - v_plus[i, j])\n",
    "            if group[i] != 1 and vars['kplus'][i, j] == 1\n",
    "            else v_plus[i, j]\n",
    "            if group[i] != 1\n",
    "            else 0\n",
    "        )\n",
    "        v_minus[i, j + 1] = (\n",
    "            v_minus[i, j] + alpha[i] * (vars['rminus'][i, j] - v_minus[i, j])\n",
    "            if group[i] != 1 and vars['kminus'][i, j] == 1\n",
    "            else v_minus[i, j]\n",
    "            if group[i] != 1\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "v_plus = pd.DataFrame(v_plus)\n",
    "v_plus = pd.melt(v_plus.iloc[:, :180])['value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lg_fun(persd=30, A=1, K=10):\n",
    "    # Initialize matrices and arrays\n",
    "    y = np.zeros((Nparticipants, Ntrials))\n",
    "    theta = np.zeros((Nparticipants, Ntrials))\n",
    "    s_plus = np.zeros((Nparticipants, Ntrials))\n",
    "    s_minus = np.zeros((Nparticipants, Ntrials))\n",
    "    v_plus = np.zeros((Nparticipants, Ntrials + 1))\n",
    "    v_minus = np.zeros((Nparticipants, Ntrials + 1))\n",
    "    g = np.zeros((Nparticipants, Ntrials))\n",
    "    d_per_p = np.zeros((Nparticipants, Ntrials))\n",
    "    d_per_m = np.zeros((Nparticipants, Ntrials))\n",
    "    \n",
    "    \n",
    "    group = np.repeat([1, 2, 3, 4], Nparticipants // 4)\n",
    "    \n",
    "    # Initialize participant-specific parameters\n",
    "    alpha = np.zeros(Nparticipants)\n",
    "    lambd = np.zeros(Nparticipants)\n",
    "    w0 = np.zeros(Nparticipants)\n",
    "    w1 = np.zeros(Nparticipants)\n",
    "    sigma = np.zeros(Nparticipants)\n",
    "    \n",
    "    \n",
    " \n",
    "    for i in range(Nparticipants):\n",
    "        alpha[i] = 0 if group[i] == 1 else np.random.beta(1, 1)\n",
    "        #set lambda[i]\n",
    "        if group[i] == 1:\n",
    "            lambd[i] = 0\n",
    "        elif group[i] == 2:\n",
    "            lambd[i] = max(0, min(0.0052, np.random.normal(0.0026, 0.001)))\n",
    "        else:\n",
    "            lambd[i] = max(0.0052, min(0.3022, np.random.normal((0.0052 + 0.3022) / 2, 0.1)))\n",
    "        w0[i] = np.random.normal(0, 5) if group[i] < 3 else np.random.normal(-2, 1)\n",
    "        w1[i] = np.random.gamma(10, 1)\n",
    "        sigma[i] = 2.5 if group[i] == 1 else 0.5\n",
    "        \n",
    "    for i in range(Nparticipants):\n",
    "        for j in range(Ntrials):\n",
    "            # Learning\n",
    "            v_plus[i, j + 1] = (\n",
    "                v_plus[i, j] + alpha[i] * (vars['rplus'][i, j] - v_plus[i, j])\n",
    "                if group[i] != 1 and vars['kplus'][i, j] == 1\n",
    "                else v_plus[i, j]\n",
    "            )\n",
    "            v_minus[i, j + 1] = (\n",
    "                v_minus[i, j] + alpha[i] * (vars['rminus'][i, j] - v_minus[i, j])\n",
    "                if group[i] != 1 and vars['kminus'][i, j] == 1\n",
    "                else v_minus[i, j]\n",
    "            )\n",
    "\n",
    "            # Similarity\n",
    "            d_per_p[i, j] = max(0, np.random.normal(vars['d_phy_p'][i, j], persd))\n",
    "            d_per_m[i, j] = max(0, np.random.normal(vars['d_phy_m'][i, j], persd))\n",
    "\n",
    "            s_plus[i, j] = (\n",
    "                np.exp(-lambd[i] * d_per_p[i, j])\n",
    "                if v_plus[i, j] > 0 and group[i] > 1 and group[i] == 4\n",
    "                else np.exp(-lambd[i] * vars['d_phy_p'][i, j])\n",
    "                if v_plus[i, j] > 0 and group[i] > 1\n",
    "                else 1\n",
    "            )\n",
    "            s_minus[i, j] = (\n",
    "                np.exp(-lambd[i] * d_per_m[i, j])\n",
    "                if abs(v_minus[i, j]) > 0 and group[i] > 1 and group[i] == 4\n",
    "                else np.exp(-lambd[i] * vars['d_phy_m'][i, j])\n",
    "                if abs(v_minus[i, j]) > 0 and group[i] > 1\n",
    "                else 1\n",
    "            )\n",
    "\n",
    "            # Generalization\n",
    "            g[i, j] = v_plus[i, j] * s_plus[i, j] + v_minus[i, j] * s_minus[i, j]\n",
    "            theta[i, j] = A + (K - A) / (1 + np.exp(-(w0[i] + w1[i] * g[i, j])))\n",
    "\n",
    "            # Response\n",
    "            y[i, j] = np.random.normal(theta[i, j], sigma[i])\n",
    "            \n",
    "       \n",
    "    rplus = [it[0] for it in  rplus_vals.values]\n",
    "    d_phy_p = [it[0] for it in  d_phy_p_vals.values]\n",
    "    d_phy_m = [it[0] for it in  d_phy_m_vals.values]\n",
    "    \n",
    "    \n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        \"participant\": np.repeat(np.arange(1, Nparticipants + 1), Ntrials),\n",
    "        \"trials\": np.tile(np.arange(1, Ntrials + 1), Nparticipants),\n",
    "        \"group\": np.repeat(group, Ntrials),\n",
    "        \"alpha\": np.repeat(alpha, Ntrials),\n",
    "        \"lambda\": np.repeat(lambd, Ntrials),\n",
    "        \"w0\": np.repeat(w0, Ntrials),\n",
    "        \"sigma\": np.repeat(sigma, Ntrials),\n",
    "        \"w1\": np.repeat(w1, Ntrials),\n",
    "        \"y\": y.flatten(),\n",
    "        \"g\": g.flatten(),\n",
    "        \"theta\": theta.flatten(),\n",
    "        \"v_plus\": v_plus[:, :Ntrials].flatten(),\n",
    "        \"v_minus\": v_minus[:, :Ntrials].flatten(),\n",
    "        \"s_plus\": s_plus.flatten(),\n",
    "        \"s_minus\": s_minus.flatten(),\n",
    "        \"d_per_p\": d_per_p.flatten(),\n",
    "        \"d_per_m\": d_per_m.flatten(),\n",
    "        \"d_phy_p\": d_phy_p,\n",
    "        \"d_phy_m\": d_phy_m,\n",
    "        \"rplus\": rplus\n",
    "    })\n",
    "\n",
    "    # Add stimulus labels\n",
    "    data[\"stim\"] = np.where(\n",
    "        data[\"d_phy_p\"] == 0, \"CS+\",\n",
    "        np.where(data[\"d_phy_m\"] == 0, \"CS-\", \"TS\")\n",
    "    )\n",
    "\n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lg_fun()\n",
    "\n",
    "\n",
    "group_labels = [\"Non-Learners\", \"Overgeneralizers\", \"Physical Generalizers\", \"Perceptual Generalizers\"]\n",
    "result['group'] = pd.Categorical(result['group'], categories=[1, 2, 3, 4], ordered=True).rename_categories(group_labels)\n",
    "result = result2.merge(data[['trials', 'stimulus']], on='trials', how='left')\n",
    "result['stimulus'] = pd.Categorical(result['stimulus'], categories=[\"CS+\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\", \"CS-\"], ordered=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visulization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Leanring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsample = np.concatenate([\n",
    "#     np.random.choice(range(1, 51), 5, replace=False),\n",
    "#     np.random.choice(range(51, 101), 5, replace=False),\n",
    "#     np.random.choice(range(101, 151), 5, replace=False),\n",
    "#     np.random.choice(range(151, 201), 5, replace=False)\n",
    "# ])\n",
    "parsample = np.array(pd.read_csv(\"../../vars/randoms/parsample.csv\").values)\n",
    "parsample = parsample.flatten()\n",
    "linecolors = {'vp': \"#e41a1c\", 'vm': \"#377eb8\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learning_plt():\n",
    "    # Filter and group the data\n",
    "    L = (result[result['trials'] < 24]\n",
    "        .groupby(['trials', 'group'])\n",
    "        .agg(mean_vp=('v_plus', 'mean'), mean_vm=('v_minus', 'mean'))\n",
    "        .reset_index())\n",
    "\n",
    "    # Set up the facet grid\n",
    "    unique_groups = result['group'].unique()\n",
    "    num_groups = len(unique_groups)\n",
    "    fig, axes = plt.subplots(num_groups, 1, figsize=(10, 6 * num_groups), sharex=True, sharey=True)\n",
    "\n",
    "    for i, (group, ax) in enumerate(zip(unique_groups, axes)):\n",
    "        # Filter data for the current group\n",
    "        group_data = result[(result['group'] == group) & (result['trials'] < 24)]\n",
    "        group_means = L[L['group'] == group]\n",
    "\n",
    "        # Plot individual lines and points for v_plus\n",
    "        sns.lineplot(data=group_data, x='trials', y='v_plus', hue='participant',\n",
    "                    palette=[\"#e41a1c\"], ax=ax, alpha=0.1, legend=None)\n",
    "        sns.scatterplot(data=group_data, x='trials', y='v_plus', hue='participant',\n",
    "                        palette=[\"#e41a1c\"], ax=ax, alpha=0.1, s=10, legend=None)\n",
    "\n",
    "        # Plot individual lines and points for v_minus\n",
    "        sns.lineplot(data=group_data, x='trials', y='v_minus', hue='participant',\n",
    "                    palette=[\"#377eb8\"], ax=ax, alpha=0.3, legend=None)\n",
    "        sns.scatterplot(data=group_data, x='trials', y='v_minus', hue='participant',\n",
    "                        palette=[\"#377eb8\"], ax=ax, alpha=0.3, s=10, legend=None)\n",
    "\n",
    "        # Plot mean lines and points for v_plus (mean_vp) and v_minus (mean_vm)\n",
    "        sns.lineplot(data=group_means, x='trials', y='mean_vp', color='red', linewidth=3, label='CS+ strength', ax=ax)\n",
    "        sns.scatterplot(data=group_means, x='trials', y='mean_vp', color='red', s=50, legend=None, ax=ax)\n",
    "        sns.lineplot(data=group_means, x='trials', y='mean_vm', color='blue', linewidth=3, label='CS- strength', ax=ax)\n",
    "        sns.scatterplot(data=group_means, x='trials', y='mean_vm', color='blue', s=50, legend=None, ax=ax)\n",
    "\n",
    "        # Shape and color for 'rplus' and 'stim'\n",
    "        for _, row in group_data.iterrows():\n",
    "            # Bottom layer for 'stim' - adjusting y position to -1.4 as in R code\n",
    "            ax.scatter(row['trials'], -1.4, color='red' if row['stim'] == 'CS+' else 'blue', \n",
    "                    s=25, marker='s', edgecolor='black')\n",
    "\n",
    "        # Customize labels and title\n",
    "        ax.set_title(group, loc='center')\n",
    "        ax.set_ylabel(\"Associative strengths\")\n",
    "\n",
    "    # Shared x-axis label\n",
    "    axes[-1].set_xlabel(\"Trials\")\n",
    "\n",
    "    # Set y-axis limits and breaks\n",
    "    plt.setp(axes, ylim=(-1.5, 1), yticks=[-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "    # Custom legend for shape encoding\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='^', color='w', label='Shock', markerfacecolor='black', markersize=8),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Shock absent', markerfacecolor='black', markersize=8),\n",
    "        Line2D([0], [0], color='red', lw=3, label='CS+ strength'),\n",
    "        Line2D([0], [0], color='blue', lw=3, label='CS- strength'),\n",
    "        Line2D([0], [0], marker='s', color='red', label='CS+ trial', markersize=8, markerfacecolor='red', markeredgecolor='black'),\n",
    "        Line2D([0], [0], marker='s', color='blue', label='CS- trial', markersize=8, markerfacecolor='blue', markeredgecolor='black')\n",
    "    ]\n",
    "    axes[0].legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    # Apply theme and styling\n",
    "    sns.set(style=\"whitegrid\", rc={\"axes.spines.right\": False, \"axes.spines.top\": False})\n",
    "\n",
    "    # Overall title\n",
    "    plt.suptitle(\"Learning\", y=1.02, fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.savefig(f\"{plts_folder}Hypothesized_Learning.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt1 = create_learning_plt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_sim_similarity_plt():\n",
    "    # Calculate max value for `d_per_p` and add 10\n",
    "    max_d = result['d_per_p'].max() + 10\n",
    "\n",
    "    # Filter rows where group is \"Non-Learners\" or (v_plus > 0.1 and v_minus < 0.1)\n",
    "    filtered_result = result[(result['group'] == \"Non-Learners\") | ((result['v_plus'] > 0.1) & (result['v_minus'] < 0.1))]\n",
    "\n",
    "    # Set up the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    g = sns.FacetGrid(filtered_result, col=\"group\", col_wrap=1, height=4, sharey=True)\n",
    "\n",
    "    # Plot each group\n",
    "    for ax, (group, group_data) in zip(g.axes.flat, filtered_result.groupby(\"group\")):\n",
    "        # Plot grey lines for s_plus as a function of `d_phy_p + 10` and `-d_per_p - 10`\n",
    "        ax.plot(group_data['d_phy_p'] + 10, group_data['s_plus'], color=\"grey\", alpha=0.25)\n",
    "        ax.plot(-group_data['d_per_p'] - 10, group_data['s_plus'], color=\"grey\", alpha=0.25)\n",
    "        \n",
    "        # Plot scatter points colored and filled by `stim`\n",
    "        sns.scatterplot(data=group_data, x=group_data['d_phy_p'] + 10, y=\"s_plus\", hue=\"stim\", \n",
    "                        style=\"stim\", palette={\"CS+\": \"#e41a1c\", \"CS-\": \"#377eb8\", \"TS\": \"grey\"}, \n",
    "                        edgecolor=\"black\", s=10, ax=ax, alpha=0.5)\n",
    "        sns.scatterplot(data=group_data, x=-group_data['d_per_p'] - 10, y=\"s_plus\", hue=\"stim\", \n",
    "                        style=\"stim\", palette={\"CS+\": \"#e41a1c\", \"CS-\": \"#377eb8\", \"TS\": \"grey\"}, \n",
    "                        edgecolor=\"black\", s=10, ax=ax, alpha=0.5)\n",
    "\n",
    "        # Add vertical line at x=0\n",
    "        ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "        ax.set_title(group)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_yticks([0, 0.5, 1])\n",
    "\n",
    "    # Customize x-axis limits and ticks\n",
    "    g.set(xlim=(-max_d, max_d))\n",
    "    x_ticks = [-max_d, -max_d/2, -10, 10, max_d/2, max_d]\n",
    "    x_labels = [round(max_d)+10, round(max_d)/2+10, 0, 0, round(max_d)/2-10, round(max_d)-10]\n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.set_xticklabels(x_labels)\n",
    "\n",
    "    # Customize labels and title\n",
    "    g.set_axis_labels(\"Distance to CS+ (left: perceptual; right: physical)\", \"Similarity to CS+\")\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    g.fig.suptitle(\"Similarity\")\n",
    "\n",
    "    # Customize legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    legend_labels = [\"CS+\", \"CS-\", \"TS\"]\n",
    "    g.add_legend(title=\"\", handles=handles[:3], labels=legend_labels, bbox_to_anchor=(1, 0.5), loc=\"center right\", frameon=False)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(f\"{plts_folder}Sim_Similarity_plus.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt2 = do_sim_similarity_plt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Generalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_generalization_all_plt():\n",
    "    G_all_data = result[result['trials'] > 24].copy()\n",
    "\n",
    "    # Group by 'stimulus' and calculate mean of 'y' for each group\n",
    "    G_all_data['mean_res'] = G_all_data.groupby('stimulus')['y'].transform('mean')\n",
    "\n",
    "    # Group by 'participant' and 'stimulus' to calculate individual mean 'y' for each group\n",
    "    G_all_data['indi_res'] = G_all_data.groupby(['participant', 'stimulus'])['y'].transform('mean')\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.lineplot(data=G_all_data, x='stimulus', y='indi_res', hue='participant',\n",
    "                palette=[\"grey\"], alpha=0.3, linewidth=1, legend=None)\n",
    "    sns.scatterplot(data=G_all_data, x='stimulus', y='indi_res', hue='participant',\n",
    "                    palette=[\"grey\"], alpha=0.3, s=10, legend=None)\n",
    "    sns.lineplot(data=G_all_data, x='stimulus', y='mean_res', color=\"black\", linewidth=1)\n",
    "    sns.scatterplot(data=G_all_data, x='stimulus', y='mean_res', color=\"black\", s=50)\n",
    "\n",
    "    # Customize plot\n",
    "    plt.xlabel(\"Stimulus\")\n",
    "    plt.ylabel(\"US Expectancy\")\n",
    "    plt.title(\"Generalized response (whole dataset)\")\n",
    "    plt.yticks([1, 5, 10])\n",
    "    plt.ylim(0, max(G_all_data['mean_res']) + 2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{plts_folder}Sim_Generalization_all.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    return plt\n",
    "plt3 = do_generalization_all_plt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_sim_generalization_plt():\n",
    "\n",
    "    # Filter rows where trials > 24\n",
    "    G_data = result[result['trials'] > 24].copy()\n",
    "\n",
    "    # Group by 'group' and 'stimulus' and calculate mean of 'y' for each group\n",
    "    G_data['mean_res'] = G_data.groupby(['group', 'stimulus'])['y'].transform('mean')\n",
    "\n",
    "    # Group by 'participant' and 'stimulus' to calculate individual mean 'y' for each group\n",
    "    G_data['indi_res'] = G_data.groupby(['participant', 'stimulus'])['y'].transform('mean')\n",
    "\n",
    "    # Faceted Plot\n",
    "    g = sns.FacetGrid(G_data, col='group', col_wrap=2, height=4, sharey=True)\n",
    "    g.map_dataframe(sns.lineplot, x='stimulus', y='indi_res', color='grey', alpha=0.3, linewidth=1)\n",
    "    g.map_dataframe(sns.scatterplot, x='stimulus', y='indi_res', color='grey', alpha=0.3, s=10)\n",
    "    g.map_dataframe(sns.lineplot, x='stimulus', y='mean_res', color=\"black\", linewidth=1)\n",
    "    g.map_dataframe(sns.scatterplot, x='stimulus', y='mean_res', color=\"black\", s=50)\n",
    "\n",
    "    # Customize plot\n",
    "    g.set_axis_labels(\"Stimulus\", \"US Expectancy\")\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    g.set(yticks=[1, 5, 10])\n",
    "    g.fig.suptitle(\"Generalized response\", y=1.05)\n",
    "    g.set(ylim=(0, max(G_data['mean_res']) + 2))\n",
    "\n",
    "    # Remove legend and adjust layout\n",
    "    plt.savefig(f\"{plts_folder}Sim_Generalization.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "plt4 = do_sim_generalization_plt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
